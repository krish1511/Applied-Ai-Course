{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm : will give you a (very fast) progress bar. You also use tqdm more explicitly,\n",
    "\n",
    "from tqdm import tqdm # tqdm is a library that helps us to visualize the runtime of for loop. refer this to know more about tqdm\n",
    "#https://tqdm.github.io/\n",
    "\n",
    "\n",
    "#fit: With this function, we will find all unique words in the data and we will assign a dimension-number to each unique word.\n",
    "\n",
    "#We will create a python dictionary to save all the unique words, such that the key of dictionary represents a unique word \n",
    "#                                       and the corresponding value represent it's dimension-number.\n",
    "\n",
    "#For example, if you have a review, __'very bad pizza'__ then you can represent each unique word with a dimension_number as,\n",
    "#dict = { 'very' : 1, 'bad' : 2, 'pizza' : 3}\n",
    "\n",
    "\n",
    "# it accepts only list of sentances\n",
    "def fit(dataset):    \n",
    "    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset, (list,)):   # it checks whether the dataset is of list data type or not\n",
    "        for row in dataset: # for each review/row in the dataset\n",
    "            #print(row)   # abc def aaa prq mn pqr aaaaaaa aaa abbb baaa\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                #print(word)  # abc          here it splits the entire word and segregate into the words in line by line\n",
    "                #             # def...\n",
    "                if len(word) < 2:       # if length of each word is less than 2 it just skips the statement\n",
    "                    continue\n",
    "                unique_words.add(word)    # it adds all the unique words into the set\n",
    "                '''print(unique_words)     # {'abc'}\n",
    "                                           # {'def', 'abc'}\n",
    "                                           # {'def', 'abc', 'aaa'}......'''\n",
    "        unique_words = sorted(list(unique_words))# here all the unique values converts into the list in sorted order\n",
    "        #print(unique_words)  #['aaa', 'aaaaaaa', 'abbb', 'abc', 'baaa', 'def', 'mn', 'pqr', 'prq']\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}#finally all the unique words with dimension number store into dict\n",
    "        #print(vocab) #{'aaa': 0, 'aaaaaaa': 1, 'abbb': 2, 'abc': 3, 'baaa': 4, 'def': 5, 'mn': 6, 'pqr': 7, 'prq': 8}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aaa': 0, 'aaaaaaa': 1, 'abbb': 2, 'abc': 3, 'baaa': 4, 'def': 5, 'mn': 6, 'pqr': 7, 'prq': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab = fit([\"abc def aaa prq mn pqr aaaaaaa aaa abbb baaa\"])  # here i stored entire in elements in the single list\n",
    "print(vocab)          # here set wont accept duplicate values. thus we have only 9 unique words from 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aaa': 0, 'aaaaaaa': 1, 'abbb': 2, 'abc': 3, 'baaa': 4, 'def': 5, 'lmn': 6, 'pqr': 7, 'prq': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab = fit([\"abc def aaa prq\", \"lmn pqr aaaaaaa aaa abbb baaa\"]) # here we have two reviews\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "88\n"
     ]
    }
   ],
   "source": [
    " # from above matrix we only have 3 non zero values so better to store it as sparse matrix rather dense matrix\n",
    "# in spasre matrix we are only storing non zero values\n",
    "from sys import getsizeof\n",
    "import numpy as np\n",
    "# we store every element here\n",
    "a = np.array([[1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 4, 0, 0]])   # dense matrix\n",
    "print(getsizeof(a))\n",
    "\n",
    "# here we are storing only non zero elements here (row, col, value)\n",
    "a = [ (0, 0, 1), (1, 3, 1), (2,2,4)]                                # sparse matrix\n",
    "# with this way of storing we are saving alomost 50% memory for this example\n",
    "print(getsizeof(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How to write a sparse matrix\n",
    "\n",
    "You can use csr_matrix() method of scipy.sparse to write a sparse matrix.\n",
    "\n",
    "You need to pass indices of non-zero elements into csr_matrix() for creating a sparse matrix.\n",
    "You also need to pass element value of each pair of indices.\n",
    "eg: [ (0, 0, 1), (1, 3, 1), (2,2,4)]   [row,column,value]\n",
    "\n",
    "You can use lists to save the indices of non-zero elements and their corresponding element values.\n",
    "\n",
    " eg: assume you have a matrix\n",
    "   [[1, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [4, 0, 6]] \n",
    "list_of_indices = [(0,0), (1,2), (2,0), (2,2)]\n",
    "And you can save the corresponding element values as,\n",
    "element_values = [1, 1, 4, 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc 2\n",
      "def 2\n",
      "zzz 2\n",
      "pqr 1\n"
     ]
    }
   ],
   "source": [
    "# Transform Method\n",
    "# With this function, we will write a feature matrix using sprase matrix.\n",
    "\n",
    "# how Counter works : dict subclass for counting hashable objects\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#How to write a sparse matrix\n",
    "#You can use csr_matrix() method of scipy.sparse to write a sparse matrix.\n",
    "\n",
    "test = 'abc def abc def zzz zzz pqr'\n",
    "#print(test.split())  #['abc', 'def', 'abc', 'def', 'zzz', 'zzz', 'pqr']\n",
    "word_frequency = dict(Counter(test.split()))\n",
    "#print(Counter(test.split()))  # Counter({'abc': 2, 'def': 2, 'zzz': 2, 'pqr': 1}) it returns occurance of words in dictionary\n",
    "#print(a) # {'abc': 2, 'def': 2, 'zzz': 2, 'pqr': 1}\n",
    "#print(a.items())   #dict_items([('abc', 2), ('def', 2), ('zzz', 2), ('pqr', 1)])\n",
    "for i,j in word_frequency.items():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB : {'but': 0, 'centerpiece': 1, 'economic': 2, 'economists': 3, 'for': 4, 'is': 5, 'its': 6, 'lagrange': 7, 'method': 8, 'multipliers': 9, 'of': 10, 'optimization': 11, 'poorly': 12, 'problems': 13, 'solving': 14, 'taught': 15, 'technique': 16, 'the': 17, 'theory': 18, 'unfortunately': 19, 'usually': 20, 'workhorse': 21}\n",
      "====================================================================================================\n",
      "VOCAB kEYS: ['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse']\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix ['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse'] [[0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 2 0 0 0 1]\n",
      " [1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0]]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 2011.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 17)\t2\n",
      "  (0, 21)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "# note that we are need to send the preprocessing text here, we have not inlcuded the processing\n",
    "\n",
    "def transform(dataset, vocab):  # [corpus(collection of reviews) , {key:value}]\n",
    "    # sparse matrix : # here we are storing only non zero elements here (row, col,elmenet_values)\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):  # checks whether the dataset is of list type or not\n",
    "        for idx,row in enumerate(tqdm(dataset)):  # for each index[dimension_number],row/review in dataset\n",
    "            # it will return a dict type object where key is the word and values is its frequency, {word:frequency},{row,index}\n",
    "            #print(idx) #0\n",
    "                        #1\n",
    "            #print(row) #the method of lagrange multipliers is the economists workhorse for solving optimization problems\n",
    "                        #the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            #print(row.split())#['the', 'method', 'of', 'lagrange', 'multipliers', 'is', 'the', 'economists', 'workhorse', 'for', 'solving', 'optimization', 'problems']\n",
    "                             #['the', 'technique', 'is', 'a', 'centerpiece', 'of', 'economic', 'theory', 'but', 'unfortunately', 'its', 'usually', 'taught', 'poorly']\n",
    "\n",
    "            #print(word_freq) #{'the': 2, 'method': 1, 'of': 1, 'lagrange': 1, 'multipliers': 1, 'is': 1, 'economists': 1, 'workhorse': 1, 'for': 1, 'solving': 1, 'optimization': 1, 'problems': 1}\n",
    "                             #{'the': 1, 'technique': 1, 'is': 1, 'a': 1, 'centerpiece': 1, 'of': 1, 'economic': 1, 'theory': 1, 'but': 1, 'unfortunately': 1, 'its': 1, 'usually': 1, 'taught': 1, 'poorly': 1}\n",
    "            \n",
    "            #print(word_freq.items()) #dict_items([('the', 2), ('method', 1), ('of', 1), ('lagrange', 1), ('multipliers', 1), ('is', 1), ('economists', 1), ('workhorse', 1), ('for', 1), ('solving', 1), ('optimization', 1), ('problems', 1)])\n",
    "                                     #dict_items([('the', 1), ('technique', 1), ('is', 1), ('a', 1), ('centerpiece', 1), ('of', 1), ('economic', 1), ('theory', 1), ('but', 1), ('unfortunately', 1), ('its', 1), ('usually', 1), ('taught', 1), ('poorly', 1)])            \n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.\n",
    "                #print(word)  #the             # each unique words\n",
    "                             #method\n",
    "                             #of\n",
    "                             #lagrange\n",
    "                             #multipliers\n",
    "                             #is\n",
    "                             #economists\n",
    "                #print(freq)  #2\n",
    "                             #1\n",
    "                             #1 ........\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in the vocabulary that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                \n",
    "                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n",
    "                                                # vocab which we give while calling the function\n",
    "                #print(col_index) #17 8 10 7 9 5 3 21 4 14 11 13  line by line not in order\n",
    "                                 #17 16 5 1 10 2 18 0 19 6 20 15 12\n",
    "                # if the word exists\n",
    "                if col_index != -1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the frequency of the word\n",
    "                    values.append(freq)  # word occurance 2 1 1 1 1 1 1 1 1\n",
    "        #return sparse matrix\n",
    "        return csr_matrix((values , (rows,columns)), shape = (len(dataset),len(vocab)) )\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "                    \n",
    "\n",
    "\n",
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit(strings)\n",
    "print('VOCAB :',vocab)\n",
    "print('='*100)\n",
    "print('VOCAB kEYS:',list(vocab.keys()))\n",
    "print('='*100)\n",
    "print('sparse matrix',list(vocab.keys()),transform(strings, vocab).toarray())\n",
    "print('='*100)\n",
    "print(transform(strings,vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit(strings)    #the    \n",
    "                        #{'the'}\n",
    "                        #method\n",
    "                        #{'the', 'method'}\n",
    "                        #of...........\n",
    "transform(strings,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB : {'but': 0, 'centerpiece': 1, 'economic': 2, 'economists': 3, 'for': 4, 'is': 5, 'its': 6, 'lagrange': 7, 'method': 8, 'multipliers': 9, 'of': 10, 'optimization': 11, 'poorly': 12, 'problems': 13, 'solving': 14, 'taught': 15, 'technique': 16, 'the': 17, 'theory': 18, 'unfortunately': 19, 'usually': 20, 'workhorse': 21}\n",
      "====================================================================================================\n",
      "VOCAB kEYS: ['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse']\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix [[0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 2 0 0 0 1]\n",
      " [1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0]]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 17)\t2\n",
      "  (0, 21)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "def transform(dataset,vocab):\n",
    "    rows=[]\n",
    "    columns= []\n",
    "    values = []\n",
    "    if isinstance(dataset ,(list,)):\n",
    "        for idx,row in enumerate(tqdm(dataset)):\n",
    "            \n",
    "            word_freq = dict(Counter(row.split())) # it gives keys and values\n",
    "            for word,freq in word_freq.items():  # each unique word and frequency\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word,-1)\n",
    "                if col_index != -1:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(col_index)\n",
    "                    values.append(freq)\n",
    "        return csr_matrix((values , (rows,columns)), shape = (len(dataset),len(vocab)) )\n",
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit(strings)\n",
    "print('VOCAB :',vocab)\n",
    "print('='*100)\n",
    "print('VOCAB kEYS:',list(vocab.keys()))\n",
    "print('='*100)\n",
    "print('Sparse Matrix',transform(strings, vocab).toarray())\n",
    "print('='*100)\n",
    "print(transform(strings,vocab))\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results with countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 2 0 0 0 1]\n",
      " [1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer='word')\n",
    "\n",
    "vec.fit(strings)\n",
    "feature_matrix_2 = vec.transform(strings)\n",
    "print(feature_matrix_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n",
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "def fit(corpus):\n",
    "    unique_words = set()\n",
    "    for row in corpus:\n",
    "        for word in row.split():\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    return vocab\n",
    "                \n",
    "vocab = fit(corpus)\n",
    "#print(vocab)    #{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
    "\n",
    "def idf(corpus,vocab):\n",
    "    idf_dict = {}\n",
    "    for word in vocab:\n",
    "        count = 0\n",
    "        for row in corpus:\n",
    "            if word in row.split(' '):\n",
    "                count +=1\n",
    "        idf_dict[word] = 1 + (math.log((1+len(corpus))/(count+1))) \n",
    "    return idf_dict\n",
    "idf_values = idf(corpus,vocab)\n",
    "#print(idf_values) #{'and': 1.916290731874155, 'document': 1.2231435513142097, 'first': 1.5108256237659907, 'is': 1.0, 'one': 1.916290731874155, 'second': 1.916290731874155, 'the': 1.0, 'third': 1.916290731874155, 'this': 1.0}\n",
    "\n",
    "def transform(corpus,vocab,idf_values):\n",
    "    sparse_matrix = csr_matrix((len(corpus),len(vocab)))\n",
    "    for row in range(0,len(corpus)):\n",
    "        number_of_words_in_sentence = dict(Counter(corpus[row].split()))\n",
    "        #print(number_of_words_in_sentence)\n",
    "        #print(corpus[row].split())\n",
    "        for word in corpus[row].split():\n",
    "            if word in list(vocab.keys()):\n",
    "                tf_idf_value = (number_of_words_in_sentence[word] / len(corpus[row].split()))*(idf_values[word])\n",
    "                #print(tf_idf_value) # it gives tf_idf_values for all the reviews in the dataset\n",
    "                sparse_matrix[row,vocab[word]] = tf_idf_value\n",
    "                #print( sparse_matrix[row,vocab[word]] ) # it gives tf_idf_values for all the reviews in the dataset\n",
    "    print(normalize(sparse_matrix,norm='l2',axis=1,copy=True,return_norm = False))\n",
    "    output = normalize(sparse_matrix,norm='l2',axis=1,copy=True,return_norm = False)\n",
    "    return output\n",
    "final_output = transform(corpus,vocab,idf_values)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
      "  (0, 1)\t0.0936585811581694\n",
      "  (0, 2)\t0.1873171623163388\n",
      "  (0, 3)\t0.2809757434745082\n",
      "  (0, 6)\t0.5619514869490164\n",
      "  (0, 8)\t0.7492686492653552\n",
      "  (1, 1)\t0.17025130615174972\n",
      "  (1, 3)\t0.2553769592276246\n",
      "  (1, 5)\t0.42562826537937426\n",
      "  (1, 6)\t0.5107539184552492\n",
      "  (1, 8)\t0.6810052246069989\n",
      "  (2, 0)\t0.0\n",
      "  (2, 3)\t0.22742941307367104\n",
      "  (2, 4)\t0.30323921743156135\n",
      "  (2, 6)\t0.4548588261473421\n",
      "  (2, 7)\t0.5306686305052324\n",
      "  (2, 8)\t0.6064784348631227\n",
      "  (3, 1)\t0.0936585811581694\n",
      "  (3, 2)\t0.1873171623163388\n",
      "  (3, 3)\t0.2809757434745082\n",
      "  (3, 6)\t0.5619514869490164\n",
      "  (3, 8)\t0.7492686492653552\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "def IDF(corpus,unique_words):\n",
    "    idf_dict = {}\n",
    "    for word in unique_words:\n",
    "        count = 0\n",
    "        for row in corpus:\n",
    "            if word in row:\n",
    "                count += 1\n",
    "            continue\n",
    "        idf = 1 + (math.log((1+len(corpus))/(count+1))) \n",
    "    idf_dict.append(idf)\n",
    "    return idf_dict\n",
    "\n",
    "def fit(corpus):\n",
    "    unique_words = set()\n",
    "    for row in corpus:\n",
    "        for word in row.split():\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    return vocab\n",
    "vocab = fit(corpus)\n",
    "print(vocab)\n",
    "\n",
    "def transform(corpus,vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(corpus,(list,)):\n",
    "        for idx,row in enumerate(tqdm(corpus)):\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            #print(word_freq)\n",
    "            for word,freq in word_freq.items():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word, -1)\n",
    "                if col_index != -1:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(col_index)\n",
    "                    tf = freq / len(row.split()) # term_frequency \n",
    "                    #𝑇𝐹(𝑡)=Number of times term t appears in a document / Total number of terms in the document/row.\n",
    "                    idf_ =  vocab[word]\n",
    "                    #𝐼𝐷𝐹(𝑡)=1+log𝑒[1 + Total number of documents in collection / 1+Number of documents with term t in it].\n",
    "                    values.append((tf) * (idf_))\n",
    "        g = normalize(csr_matrix((values, (rows,columns)), shape=(len(corpus),len(vocab))),norm='l2',axis=1,copy = True,return_norm= False)\n",
    "        print(vocab)\n",
    "        return g\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "    \n",
    "final_output = transform(corpus,vocab)  \n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aailiyah', 'abandoned', 'ability', 'abroad', 'absolutely', 'abstruse', 'abysmal', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'accused', 'achievement', 'achille', 'ackerman', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'actually', 'adams', 'adaptation', 'add', 'added', 'addition', 'admins', 'admiration', 'admitted', 'adorable', 'adrift', 'adventure', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'afraid', 'africa', 'afternoon', 'age', 'aged', 'ages']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names()[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.922918   6.922918   6.22977082 ... 6.922918   6.5174529  6.922918  ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 2886)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2878)\t0.35781145622317734\n",
      "  (0, 2287)\t0.3377679916467555\n",
      "  (0, 1653)\t0.35781145622317734\n",
      "  (0, 1651)\t0.16192317905848022\n",
      "  (0, 1545)\t0.30566026894803877\n",
      "  (0, 720)\t0.4123943870778812\n",
      "  (0, 688)\t0.4123943870778812\n",
      "  (0, 53)\t0.4123943870778812\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aailiyah': 6.922918004572872, 'abandoned': 6.922918004572872, 'abroad': 6.922918004572872, 'abstruse': 6.922918004572872, 'academy': 6.922918004572872, 'accents': 6.922918004572872, 'accessible': 6.922918004572872, 'acclaimed': 6.922918004572872, 'accolades': 6.922918004572872, 'accurate': 6.922918004572872, 'accurately': 6.922918004572872, 'achille': 6.922918004572872, 'ackerman': 6.922918004572872, 'actions': 6.922918004572872, 'adams': 6.922918004572872, 'add': 6.922918004572872, 'added': 6.922918004572872, 'admins': 6.922918004572872, 'admiration': 6.922918004572872, 'admitted': 6.922918004572872, 'adrift': 6.922918004572872, 'adventure': 6.922918004572872, 'aesthetically': 6.922918004572872, 'affected': 6.922918004572872, 'affleck': 6.922918004572872, 'afternoon': 6.922918004572872, 'aged': 6.922918004572872, 'ages': 6.922918004572872, 'agree': 6.922918004572872, 'agreed': 6.922918004572872, 'aimless': 6.922918004572872, 'aired': 6.922918004572872, 'akasha': 6.922918004572872, 'akin': 6.922918004572872, 'alert': 6.922918004572872, 'alike': 6.922918004572872, 'allison': 6.922918004572872, 'allow': 6.922918004572872, 'allowing': 6.922918004572872, 'alongside': 6.922918004572872, 'amateurish': 6.922918004572872, 'amaze': 6.922918004572872, 'amazed': 6.922918004572872, 'amazingly': 6.922918004572872, 'amusing': 6.922918004572872, 'amust': 6.922918004572872, 'anatomist': 6.922918004572872, 'angel': 6.922918004572872, 'angela': 6.922918004572872, 'angelina': 6.922918004572872}\n",
      "  (0, 30)\t1.0\n",
      "  (68, 24)\t1.0\n",
      "  (72, 29)\t1.0\n",
      "  (74, 31)\t1.0\n",
      "  (119, 33)\t1.0\n",
      "  (135, 3)\t0.37796447300922725\n",
      "  (135, 10)\t0.37796447300922725\n",
      "  (135, 18)\t0.37796447300922725\n",
      "  (135, 20)\t0.37796447300922725\n",
      "  (135, 36)\t0.37796447300922725\n",
      "  (135, 40)\t0.37796447300922725\n",
      "  (135, 41)\t0.37796447300922725\n",
      "  (176, 49)\t1.0\n",
      "  (181, 13)\t1.0\n",
      "  (192, 21)\t1.0\n",
      "  (193, 23)\t1.0\n",
      "  (216, 2)\t1.0\n",
      "  (222, 47)\t1.0\n",
      "  (225, 19)\t1.0\n",
      "  (227, 17)\t1.0\n",
      "  (241, 44)\t1.0\n",
      "  (270, 1)\t1.0\n",
      "  (290, 25)\t1.0\n",
      "  (333, 26)\t1.0\n",
      "  (334, 15)\t1.0\n",
      "  (341, 43)\t1.0\n",
      "  (344, 42)\t1.0\n",
      "  (348, 8)\t1.0\n",
      "  (377, 37)\t1.0\n",
      "  (409, 5)\t1.0\n",
      "  (430, 39)\t1.0\n",
      "  (457, 45)\t1.0\n",
      "  (461, 4)\t1.0\n",
      "  (465, 38)\t1.0\n",
      "  (475, 35)\t1.0\n",
      "  (493, 6)\t1.0\n",
      "  (500, 48)\t1.0\n",
      "  (548, 0)\t0.7071067811865475\n",
      "  (548, 32)\t0.7071067811865475\n",
      "  (608, 14)\t1.0\n",
      "  (612, 11)\t1.0\n",
      "  (620, 46)\t1.0\n",
      "  (632, 7)\t1.0\n",
      "  (644, 12)\t0.7071067811865475\n",
      "  (644, 27)\t0.7071067811865475\n",
      "  (664, 28)\t1.0\n",
      "  (667, 22)\t1.0\n",
      "  (691, 34)\t1.0\n",
      "  (697, 9)\t1.0\n",
      "  (722, 16)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "\n",
    "def fit(corpus):\n",
    "    unique_words = set()\n",
    "    for row in corpus:\n",
    "        #print(row)\n",
    "        for word in row.split(' '):\n",
    "            #print(word)\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    return vocab\n",
    "                \n",
    "vocab = fit(corpus)\n",
    "#print(vocab)\n",
    "\n",
    "def IDF(corpus,vocab):\n",
    "    idf_dict = {}\n",
    "    idf_50 = []\n",
    "    dic = {}\n",
    "    for word in vocab:\n",
    "        count = 0\n",
    "        for row in corpus:\n",
    "            if word in row.split(' '):\n",
    "                count += 1\n",
    "        idf_dict[word] = 1 + (math.log((1+len(corpus))/(count+1)))\n",
    "    #print(idf_dict.items())\n",
    "    idf_50 = sorted(idf_dict.items(),reverse = True , key = lambda x:x[1]) \n",
    "    idf_50 = idf_50[:50]\n",
    "    dic = dict(idf_50)\n",
    "    return dic\n",
    "idf_values = IDF(corpus,vocab)\n",
    "print(idf_values)            \n",
    "\n",
    "def transform(corpus,idf_values):\n",
    "    sparse_matrix = csr_matrix((len(corpus),len(idf_values)))\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for key in idf_values.keys():\n",
    "        l1.append(key)\n",
    "    for row in range(0,len(corpus)):\n",
    "        number_of_words_in_sentence = dict(Counter(corpus[row].split()))\n",
    "        for word in corpus[row].split():\n",
    "            if word in list(idf_values.keys()):\n",
    "                l2 = {j:i for i,j in enumerate(l1)}\n",
    "                col_index = l2.get(word,-1)\n",
    "                if col_index != -1:\n",
    "                    tf_idf_value = (number_of_words_in_sentence[word] / len(corpus[row].split()))*(idf_values[word])\n",
    "                    sparse_matrix[row,col_index] = tf_idf_value\n",
    "    #print(normalize(sparse_matrix,norm='l2',axis=1,copy=True,return_norm = False))\n",
    "    output = normalize(sparse_matrix,norm='l2',axis=1,copy=True,return_norm = False)\n",
    "    return output\n",
    "            #print(l2)\n",
    "final_output = transform(corpus,idf_values)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer='word')\n",
    "\n",
    "vec.fit(corpus)\n",
    "feature_matrix_2 = vec.transform(corpus)\n",
    "print(feature_matrix_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature     tfidf\n",
      "0        zombiez  6.922918\n",
      "1      havilland  6.922918\n",
      "2         hearts  6.922918\n",
      "3          heads  6.922918\n",
      "4            hbo  6.922918\n",
      "5       hayworth  6.922918\n",
      "6          hayao  6.922918\n",
      "7            hay  6.922918\n",
      "8         hatred  6.922918\n",
      "9          heche  6.922918\n",
      "10        harris  6.922918\n",
      "11         happy  6.922918\n",
      "12     happiness  6.922918\n",
      "13         hanks  6.922918\n",
      "14       hankies  6.922918\n",
      "15          hang  6.922918\n",
      "16  heartwarming  6.922918\n",
      "17         heels  6.922918\n",
      "18          gone  6.922918\n",
      "19          hero  6.922918\n",
      "20        higher  6.922918\n",
      "21          hide  6.922918\n",
      "22           hes  6.922918\n",
      "23       heroism  6.922918\n",
      "24       heroine  6.922918\n",
      "25        heroes  6.922918\n",
      "26     hernandez  6.922918\n",
      "27         heist  6.922918\n",
      "28    hendrikson  6.922918\n",
      "29       helping  6.922918\n",
      "30          help  6.922918\n",
      "31         helms  6.922918\n",
      "32       hellish  6.922918\n",
      "33         helen  6.922918\n",
      "34       handles  6.922918\n",
      "35        handle  6.922918\n",
      "36           ham  6.922918\n",
      "37         grade  6.922918\n",
      "38        grates  6.922918\n",
      "39         grasp  6.922918\n",
      "40      graphics  6.922918\n",
      "41       granted  6.922918\n",
      "42        grainy  6.922918\n",
      "43     gradually  6.922918\n",
      "44    government  6.922918\n",
      "45       halfway  6.922918\n",
      "46        gotten  6.922918\n",
      "47         gotta  6.922918\n",
      "48          goth  6.922918\n",
      "49          gosh  6.922918\n"
     ]
    }
   ],
   "source": [
    "# sklearn implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "\n",
    "topn_ids = np.argsort(vectorizer.idf_)[::-1]\n",
    "features = vectorizer.get_feature_names()\n",
    "top_n = 50\n",
    "top_feats = [(features[i],vectorizer.idf_[i]) for i in topn_ids[:top_n]]  # topfeatures = features,tfidf values and iterates till 25\n",
    "df = pd.DataFrame(top_feats)              # creating a dataframe of top_feats\n",
    "df.columns = ['feature','tfidf']          # creating the columns\n",
    "print(df)\n",
    "\n",
    "\n",
    "def top_tfidf_features(row, features, top_n=200):\n",
    "    # get top n tfidf values in row and return them with their corresponding ranks\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]#sorting the top 25 in descending order of review given in calling function, till 25\n",
    "    #np.argsort(row)[::-1] # sorting the row according to given in calling function[which review/row from descending order]\n",
    "    #np.argsort(row)[::-1][:top_n] == it sort the row till 25 values\n",
    "    top_feats = [(features[i],row[i]) for i in topn_ids]  # topfeatures = features,tfidf values and iterates till 25\n",
    "    df = pd.DataFrame(top_feats)              # creating a dataframe of top_feats\n",
    "    df.columns = ['feature','tfidf']          # creating the columns\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_features(final_output[0,:].toarray()[0],features,200)\n",
    "\n",
    "# row = i had get the review 1 and converted into sparse matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))  # (unigram, bigram)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(corpus)\n",
    "\n",
    "# final_tf_idf itslef is a sparse matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 9290)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()\n",
    "#during bow we had 115k dimensions[uni-gram] now we have 2.9M dimensions [massive]\n",
    "\n",
    "#2.9M features/dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9290"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aailiyah',\n",
       " 'aailiyah pretty',\n",
       " 'abandoned',\n",
       " 'abandoned factory',\n",
       " 'ability',\n",
       " 'ability dwight',\n",
       " 'ability meld',\n",
       " 'ability pull',\n",
       " 'abroad',\n",
       " 'abroad interacting']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0:10]   #bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# convert a row in sparsematrix to a numpy array\n",
    "# for review3 if i wanna get the vector\n",
    "print(final_tf_idf[3,:].toarray()[0])   #if want to get review 3 into sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=25):\n",
    "    # get top n tfidf values in row and return them with their corresponding ranks\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]#sorting the top 25 in descending order of review given in calling function, till 25\n",
    "    #np.argsort(row)[::-1] # sorting the row according to given in calling function[which review/row from descending order]\n",
    "    #np.argsort(row)[::-1][:top_n] == it sort the row till 25 values\n",
    "    top_feats = [(features[i],row[i]) for i in topn_ids]  # topfeatures = features,tfidf values and iterates till 25\n",
    "    df = pd.DataFrame(top_feats)              # creating a dataframe of top_feats\n",
    "    df.columns = ['feature','tfidf']          # creating the columns\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_features(final_output[0,:].toarray()[0],features,25)\n",
    "\n",
    "# row = i had get the review 1 and converted into sparse matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feature  tfidf\n",
      "0           add    1.0\n",
      "1          ages    0.0\n",
      "2      accurate    0.0\n",
      "3        action    0.0\n",
      "4        acting    0.0\n",
      "5         acted    0.0\n",
      "6           act    0.0\n",
      "7      ackerman    0.0\n",
      "8       achille    0.0\n",
      "9   achievement    0.0\n",
      "10      accused    0.0\n",
      "11   accurately    0.0\n",
      "12    accolades    0.0\n",
      "13        actor    0.0\n",
      "14    acclaimed    0.0\n",
      "15   accessible    0.0\n",
      "16      accents    0.0\n",
      "17      academy    0.0\n",
      "18      abysmal    0.0\n",
      "19     abstruse    0.0\n",
      "20   absolutely    0.0\n",
      "21       abroad    0.0\n",
      "22      ability    0.0\n",
      "23    abandoned    0.0\n",
      "24      actions    0.0\n"
     ]
    }
   ],
   "source": [
    "print(top_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
