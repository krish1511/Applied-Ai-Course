{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n",
    "\n",
    "Number of reviews: 568,454\n",
    "Number of users: 256,059\n",
    "Number of products: 74,258\n",
    "Timespan: Oct 1999 - Oct 2012\n",
    "Number of Attributes/Columns in data: 10\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Id\n",
    "ProductId - unique identifier for the product\n",
    "UserId - unqiue identifier for the user\n",
    "ProfileName\n",
    "HelpfulnessNumerator - number of users who found the review helpful\n",
    "HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "Score - rating between 1 and 5\n",
    "Time - timestamp for the review\n",
    "Summary - brief summary of the review\n",
    "Text - text of the review\n",
    "Objective:\n",
    "Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "\n",
    "[Q] How to determine if a review is positive or negative?\n",
    "\n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk          #natural language processing tool kit : for processing Text\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#scikitlearn library for machine learning\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('./amazon-fine-food-reviews/database.sqlite') \n",
    "filtered_data = pd.read_sql_query('''\n",
    "SELECT * \n",
    "FROM Reviews\n",
    "WHERE Score !=3\n",
    "''',con)\n",
    "\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition)\n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of data points in our data (525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('no of data points in our data',filtered_data.shape)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning : Deduplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query('''\n",
    "Select * FROM Reviews\n",
    "WHERE Score!=3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductId\n",
    "''',con)\n",
    "display.head()\n",
    "# single user should have given review for the single product. products would have same. but model would be different\n",
    "# so we should dedupe the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Sorting data according to ProductId in ascending order\n",
    "\n",
    "sorted_data = filtered_data.sort_values('ProductId',axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365333, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once sorting is done next deduplications of entries\n",
    "\n",
    "final = sorted_data.drop_duplicates(subset={'UserId','ProfileName','Time','Summary','Text'}, keep='first', inplace=False)\n",
    "final.shape\n",
    "\n",
    "# 365333 rows and 10 columns\n",
    "# after 568,454 after dropping duplicates 365333 is remained\n",
    "\n",
    "# after filtered_data reviews were 568457 after final filtering data 365333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.4795117665182"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0) / (filtered_data['Id'].size*1.0)*100   # 365333/ 568454 *100\n",
    "\n",
    "# 69% data remained after we cleaned up the data by removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:- It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HelpfullnessDenominator > HelpfullnessNumerator\n",
    "\n",
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT * \n",
    "FROM Reviews\n",
    "WHERE Score !=3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\",con)\n",
    "display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365331, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    307967\n",
       "0     57364\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "final['Score'].value_counts()\n",
    "\n",
    "# positve reviews:307967  and negative reviews:57364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "# find sentence containing HTML tags\n",
    "# Alphanumeric : a character that is either a letter or a number.\n",
    "\n",
    "i=0\n",
    "for sent in final['Text'].values:       # for each review / sentence\n",
    "    if (len(re.findall('<.*>',sent))):# if regular expression of finding that special characters print them along with the count\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mightn', \"shan't\", 'hadn', \"haven't\", 'are', 'and', 'such', 'had', 'for', 'they', 'during', 'themselves', 'each', 's', 'under', 'out', 'shouldn', \"wouldn't\", \"you're\", 'here', 'his', 'of', 'doesn', 'again', 'my', 'couldn', 'itself', 'but', 'did', 'with', 'were', 'won', 'over', 't', 'both', 'to', 'our', 'no', 'yourself', 'at', 'wouldn', 'by', 'further', 'yours', 'in', 'all', \"don't\", 'so', 'which', 'hasn', 'now', 'against', 'ours', 'before', 'as', \"she's\", 'hers', 'an', 'not', \"mustn't\", \"you'd\", 'below', 'off', 'should', 'few', 'she', 'or', 'theirs', 'will', 'than', \"you'll\", 'you', 'only', 'd', 'didn', 'other', \"shouldn't\", 'this', 'a', 'above', 'm', 'nor', 'shan', 'very', \"that'll\", 'myself', \"wasn't\", 'her', 'while', 'most', 'he', 'ourselves', 'down', 'there', \"should've\", 'on', 'me', 'where', 'whom', 'who', 'after', \"isn't\", 'll', 'same', 'do', \"it's\", 'its', 'into', 'have', 'once', \"weren't\", 'isn', 'i', 'aren', 'then', 'yourselves', 'just', 'these', \"couldn't\", 'ain', 'between', \"doesn't\", \"you've\", 'what', 'ma', 'up', 've', 'your', 'them', 'their', 'has', 'doing', 'some', 'weren', 'can', 'does', 'the', 'be', 'why', 'haven', 'being', \"won't\", 'when', \"didn't\", 'needn', 'was', 'been', 'himself', 'how', 'more', 'herself', 'am', \"needn't\", 're', 'y', 'having', \"aren't\", 'o', 'any', 'mustn', \"mightn't\", 'is', \"hasn't\", 'we', 'that', 'it', 'about', 'from', 'too', 'him', 'own', \"hadn't\", 'those', 'if', 'until', 'wasn', 'don', 'because', 'through'}\n",
      "====================================================================================================\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) # set of stopwords\n",
    "#stop_words = stop_words.remove('not')\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english') # initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(s):\n",
    "    a=re.sub('[|>.*?|\\.*|?.*?]',\"\",s)\n",
    "    return a\n",
    "def cleanpunc(s):\n",
    "    a=re.sub('[.|,|!,|)|(|/|\\|”|\\’|#|@|$|-|%|]',\"\",s)  # substitue means replace with space\n",
    "    return a\n",
    "\n",
    "print(stop_words)\n",
    "print('='*100)\n",
    "print(snowball_stemmer.stem('tasty'))  # stemming drops last two characters\n",
    "\n",
    "# here i have removed html tags, punctutaions and special charcters etc., and gave the stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps\n",
    "\n",
    "i = 0\n",
    "str1 = ' '\n",
    "final_string =[]\n",
    "all_positive_words=[]    # store words from positive reviews here\n",
    "all_negative_words=[]    # store words from negative reviews here\n",
    "s=''\n",
    "for sentence in final['Text'].values:\n",
    "    filtered_sentence = []  # after all the cleaning is done im going to store it in filtered_sentence\n",
    "    #print(sent)\n",
    "    sentence = cleanhtml(sentence)  # remove html tags\n",
    "    for words in sentence.split():       # it splits all the words in the sentence and into the words\n",
    "        for cleaned_words in cleanpunc(words).split(): # it splits the removal special characters wordsafter cleaning\n",
    "            if ((cleaned_words.isalpha())) & (len(cleaned_words)>2): # if cleaned_words are either a letter or number\n",
    "                                                                    # and each cleaned words length is greater than 2\n",
    "                if (cleaned_words.lower() not in stop_words):#the lowercase cleaned_words which are not present in stopwords\n",
    "                    s = (snowball_stemmer.stem(cleaned_words.lower())).encode('utf8')\n",
    "                        # after stemming the lower case cleaned_words eg: taste,tasti\n",
    "                    filtered_sentence.append(s)  # append stemmed cleaned_words into filtered_sentence\n",
    "                        \n",
    "                    if (final['Score'].values)[i] == 1:\n",
    "                        all_positive_words.append(s)          # list of all positve words appending into all_positive_words\n",
    "                    if (final['Score'].values)[i] == 0:\n",
    "                        all_negative_words.append(s)          # list of all positve words appending into all_positive_words\n",
    "                else:\n",
    "                    continue  # if lower case cleaned_words are present in stopwords then skip\n",
    "            else:\n",
    "                continue # if cleaned_words are not alpha numeric and not len of cleaned_words are greater than 2 then continue\n",
    "                \n",
    "    str1 = b' '.join(filtered_sentence) # final string of cleaned words      b is bytes\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText'] = final_string   # adding a column of CleandText which stores the appending values of final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(3)\n",
    "\n",
    "\n",
    "# store final table into sqllite table for future\n",
    "\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews',conn, schema = None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations : 1. a new column has been added to our dataset. which is CleanedText from which we have the filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW\n",
    "# Countvectorizer() in scikit-learn\n",
    "# converting word to a vector is called vectorizer\n",
    "# By using CountVectorizer function we can convert text document to matrix of word count.\n",
    "#eg: review ==> unique words  ==> 1,2,3,4,5 ==> 11010 etc.,\n",
    "# Matrix which is produced here is sparse matrix. \n",
    "\n",
    "#CountVectorizer converts text document to matrix of word count[sparse matrix]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)\n",
    "\n",
    "#in final dataframe at Text column ,get the Text column convert them into values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365331, 115281)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()         #365331 rows/reviews  and 115281 columns/words in document\n",
    "                                 # every column here corresponds to an unqiue word\n",
    "\n",
    "#sparse matrix = 365331 * 115281\n",
    "#115281 unique words ; every word has different dimensioanlity\n",
    "\n",
    "\n",
    "\n",
    "# Total -elements --> n*m = 365331 * 1152581 = 64803042 words\n",
    "# total non-zero elements ==> k = 156497\n",
    "\n",
    "# sparsity ==> s= (n*m-k)/(n*m) = (64803042 - 156497) / 64803042 = 0.9975\n",
    "# Density ==> D = (k/n*m) or (1-sparsity) = 156497/64803042 = 0.0025\n",
    "# Reduction = (n*m)/(3* no of non zero elements) = 64803042/3*156497 = 138 times\n",
    "\n",
    "# sparsity is no of 0 values. \n",
    "#Note — Countvectorizer produces sparse matrix which sometime not suited for some machine learning model \n",
    "#hence first convert this sparse matrix to dense matrix then apply machine learning model\n",
    "\n",
    "#thus, dense = 1 - sparsity = 1-0.9975 = 0.0025  \n",
    "# dense matrix is no of non zero values\n",
    "# Hence dense matrix is suited for machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['CleanedText'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365331, 114602)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()   #after all the cleaning is done  we came to 114602 words [dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni-gram, Bi-gram and N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After which we collect the words used to describe positive and negative reviews\n",
    "\n",
    "#now that we have our list of words describing positive and negative reviews lets analyse them.\n",
    "# we begin analysis by getting the frequency distribution of the words as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common positive words: [(b'like', 139093), (b'tast', 126408), (b'good', 110151), (b'love', 107082), (b'flavor', 106234), (b'use', 103384), (b'great', 102388), (b'one', 95384), (b'product', 88636), (b'tri', 85869)]\n",
      "Most common Negative words: [(b'tast', 33853), (b'like', 32189), (b'product', 27476), (b'one', 20283), (b'flavor', 18603), (b'would', 18023), (b'tri', 17670), (b'use', 15246), (b'good', 14572), (b'coffe', 14158)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive = nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
    "print('Most common positive words:',freq_dist_positive.most_common(10))\n",
    "print('Most common Negative words:',freq_dist_negative.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: From the above it can be seen that the most common positive and negative words overlap \n",
    "for eg: 'like' could be used as 'not like' etc.,\n",
    "\n",
    "so it is a good idea to consider pairs of consequent words(bi-grams) or q sequence of n consecutive words(n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-gram , tri-gram and n-gram\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#ngram_range=(1, 1),(min_n , max_n)\n",
    "count_vect = CountVectorizer(ngram_range=(1,2)) # in scikit learn  ; the words ranges between uni-gram and bi-gram gives massivecount\n",
    " # (uni-gram, bi-gram) # as n increases, d[dimensions] increases \n",
    "final_bigram_counts = count_vect.fit_transform(final['CleanedText'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365331, 2958327)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_counts.get_shape()\n",
    "\n",
    "#during bow we had 115k dimensions[uni-gram] now we have 2.9M dimensions [massive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (term frequency - inverse documnet frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))  # (unigram, bigram)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "# final_tf_idf itslef is a sparse matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365331, 2958327)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()\n",
    "#during bow we had 115k dimensions[uni-gram] now we have 2.9M dimensions [massive]\n",
    "\n",
    "#2.9M features/dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958327"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anoth method',\n",
       " 'anoth metra',\n",
       " 'anoth metro',\n",
       " 'anoth middl',\n",
       " 'anoth might',\n",
       " 'anoth migrain',\n",
       " 'anoth mild',\n",
       " 'anoth milder',\n",
       " 'anoth mile',\n",
       " 'anoth milk']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[100000:100010]   #bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# convert a row in sparsematrix to a numpy array\n",
    "# for review3 if i wanna get the vector\n",
    "print(final_tf_idf[3,:].toarray()[0])   #if want to get review 3 into sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=25):\n",
    "    # get top n tfidf values in row and return them with their corresponding ranks\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]#sorting the top 25 in descending order of review given in calling function, till 25\n",
    "    #np.argsort(row)[::-1] # sorting the row according to given in calling function[which review/row from descending order]\n",
    "    #np.argsort(row)[::-1][:top_n] == it sort the row till 25 values\n",
    "    top_feats = [(features[i],row[i]) for i in topn_ids]  # topfeatures = features,tfidf values and iterates till 25\n",
    "    df = pd.DataFrame(top_feats)              # creating a dataframe of top_feats\n",
    "    df.columns = ['feature','tfidf']          # creating the columns\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_features(final_tf_idf[1,:].toarray()[0],features,25)\n",
    "\n",
    "# row = i had get the review 1 and converted into sparse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>version paperback</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>page open</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>grew read</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>incorpor love</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>keep page</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>read sendak</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>sendak book</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>rosi movi</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>movi incorpor</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>paperback seem</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>flimsi take</td>\n",
       "      <td>0.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>realli rosi</td>\n",
       "      <td>0.186102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>cover version</td>\n",
       "      <td>0.186102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>miss hard</td>\n",
       "      <td>0.181890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>kind flimsi</td>\n",
       "      <td>0.175953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.175953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>sendak</td>\n",
       "      <td>0.173696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>book watch</td>\n",
       "      <td>0.170016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>watch realli</td>\n",
       "      <td>0.168474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>howev miss</td>\n",
       "      <td>0.164632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>paperback</td>\n",
       "      <td>0.161592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>hand keep</td>\n",
       "      <td>0.152884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>two hand</td>\n",
       "      <td>0.149718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>rosi</td>\n",
       "      <td>0.148546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>love son</td>\n",
       "      <td>0.146450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0   version paperback  0.192039\n",
       "1           page open  0.192039\n",
       "2           grew read  0.192039\n",
       "3       incorpor love  0.192039\n",
       "4           keep page  0.192039\n",
       "5         read sendak  0.192039\n",
       "6         sendak book  0.192039\n",
       "7           rosi movi  0.192039\n",
       "8       movi incorpor  0.192039\n",
       "9      paperback seem  0.192039\n",
       "10        flimsi take  0.192039\n",
       "11        realli rosi  0.186102\n",
       "12      cover version  0.186102\n",
       "13          miss hard  0.181890\n",
       "14        kind flimsi  0.175953\n",
       "15         hard cover  0.175953\n",
       "16             sendak  0.173696\n",
       "17         book watch  0.170016\n",
       "18       watch realli  0.168474\n",
       "19         howev miss  0.164632\n",
       "20          paperback  0.161592\n",
       "21          hand keep  0.152884\n",
       "22           two hand  0.149718\n",
       "23               rosi  0.148546\n",
       "24           love son  0.146450"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf      # feature is bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF still doesnt take semantic word so that is why we use word2vec : tasty ==> delicious ; cheap==> affordable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes semantic meaning of words into consideration\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary = True)\n",
    "# google gives you the vector representation in the form of above link and vector representatition is 300 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76640123"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.021405596"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','Aeroplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('men', 0.767493724822998),\n",
       " ('Women', 0.7283450365066528),\n",
       " ('womens', 0.6786180138587952),\n",
       " ('girls', 0.633903980255127),\n",
       " ('females', 0.6240420937538147),\n",
       " ('mothers', 0.6050933599472046),\n",
       " ('ladies', 0.5865179300308228),\n",
       " ('husbands', 0.5705342292785645),\n",
       " ('transwomen', 0.5697939991950989),\n",
       " ('Men', 0.5693342685699463)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('robot', 0.8341808319091797),\n",
       " ('Robots', 0.7578040361404419),\n",
       " ('robotic', 0.7340320348739624),\n",
       " ('autonomous_robots', 0.7151706218719482),\n",
       " ('humanoid_robots', 0.7042970657348633),\n",
       " ('robotics', 0.6718464493751526),\n",
       " ('Robot', 0.6514714956283569),\n",
       " ('bots', 0.6371974349021912),\n",
       " ('humanoid', 0.6353020668029785),\n",
       " ('androids', 0.6327425837516785)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('robots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Intelligence', 0.7189884185791016),\n",
       " ('intel', 0.6356417536735535),\n",
       " ('CIA', 0.6148777008056641),\n",
       " ('counterintelligence', 0.604588508605957),\n",
       " ('Alain_Chouet', 0.5940318703651428),\n",
       " ('Intelligence_Agency', 0.5846039056777954),\n",
       " ('counterterrorism', 0.5823408365249634),\n",
       " ('humint', 0.5769444108009338),\n",
       " ('chief_Ali_Mamluk', 0.5650478005409241),\n",
       " ('traditional_spycraft', 0.5622859001159668)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.8730389475822449),\n",
       " ('scrumptious', 0.8007042407989502),\n",
       " ('yummy', 0.7856924533843994),\n",
       " ('flavorful', 0.7420164346694946),\n",
       " ('delectable', 0.7385421991348267),\n",
       " ('juicy_flavorful', 0.7114803791046143),\n",
       " ('appetizing', 0.7017217874526978),\n",
       " ('crunchy_salty', 0.7012300491333008),\n",
       " ('flavourful', 0.6912213563919067),\n",
       " ('flavoursome', 0.6857702732086182)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.most_similar('tasti')  # it gives you an error\n",
    "model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45559818"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.similarity('taste','tasti') # it gives you an error becoz it wont always accept the stemming words [meaningless]\n",
    "model.wv.similarity('taste','tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own word2vec model using your own text corpus\n",
    "import gensim\n",
    "i = 0\n",
    "list_of_sentence = []\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence = []      # after all the cleaning is done im going to store it in filtered_sentence\n",
    "    sentence = cleanhtml(sent)    # removes the html\n",
    "    \n",
    "    for words in sent.split(): # it splits all cleaned words in the sentence and into the words\n",
    "        for cleaned_words in cleanpunc(words).split(): # it splits the removal special characters wordsafter cleaning\n",
    "            if ((cleaned_words.isalpha())) & (len(cleaned_words)>2): # if cleaned_words are either a letter or number\n",
    "                                                                    # and each cleaned words length is greater than 2\n",
    "                filtered_sentence.append(cleaned_words.lower())#it adds all the lower case cleaned words into filtered_sentence \n",
    "            else:\n",
    "                continue   # if they are not a letter or a number it just skips\n",
    "    list_of_sentence.append(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
      "====================================================================================================\n",
      "['this', 'witty', 'little', 'book', 'makes', 'son', 'laugh', 'loud', 'recite', 'the', 'car', 'driving', 'along', 'and', 'always', 'can', 'sing', 'the', 'refrain', 'learned', 'about', 'whales', 'india', 'drooping', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'all', 'this', 'classic', 'book', 'willing', 'bet', 'son', 'will', 'still', 'able', 'recite', 'from', 'memory', 'when', 'college']\n"
     ]
    }
   ],
   "source": [
    "print(final['Text'].values[0])\n",
    "print('='*100)\n",
    "print(list_of_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "# To train the word2Vec model\n",
    "w2v_model= gensim.models.Word2Vec(list_of_sentence,min_count=5,size=50,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33016\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.8855271339416504),\n",
       " ('yummy', 0.8363196849822998),\n",
       " ('delicious', 0.8146854639053345),\n",
       " ('satisfying', 0.8130784034729004),\n",
       " ('filling', 0.7824431657791138),\n",
       " ('flavorful', 0.7773221731185913),\n",
       " ('scrumptious', 0.6963809132575989),\n",
       " ('versatile', 0.6841164231300354),\n",
       " ('hardy', 0.6776205897331238),\n",
       " ('delicous', 0.6768090724945068)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasty')  # using my corpus earlier i have used google corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resemble', 0.7000719308853149),\n",
       " ('prefer', 0.6620005965232849),\n",
       " ('alright', 0.6514576077461243),\n",
       " ('gross', 0.6503801345825195),\n",
       " ('weird', 0.6427991390228271),\n",
       " ('dislike', 0.6390759944915771),\n",
       " ('okay', 0.6293050646781921),\n",
       " ('fake', 0.6194825768470764),\n",
       " ('hate', 0.6076527833938599),\n",
       " ('remind', 0.6031914949417114)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466718\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "count_feature_vect = count_vect.get_feature_names()\n",
    "print(count_feature_vect.index('like'))\n",
    "print(count_feature_vect[1466718])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Word2vec and TFIDF Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word2vec\n",
    "#w2v : word==> vector(d-dim)\n",
    "#AvgW2v : sequence of words/sentence --> vector\n",
    "\n",
    "# n1words ==> review1: w1 w2 w1 w3 w4 w5\n",
    "# review1 ==> vector1\n",
    "# AvgW2v: 1/n1 [w2v(w1) + w2v(w2) + w2v(w1) + w2v(w3) + w2v(w4) + w2v(w5)]\n",
    "#AvgW2v: 1/n sum of w2v wi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf word2vec\n",
    "# review1 : w1 w2 w1 w3 w4 w5\n",
    "#tfidf :   |t1|t2|t3|t4|t5|t6|   # while converting word to vector values has been given in the box\n",
    "\n",
    "#tfidf-w2v(review1) = t1*w2v(w1)+t2*w2v(w2)+t3*w2v(w3)+t4*w2v(w4)+t5*w2v(w5)  //  (t1+t2+t3+t4+t5)\n",
    "\n",
    "#tfidf-w2v(review1)  = sumof (ti * w2v(wi)) // sum of ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365331\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#average word2vec\n",
    "# compute average word2vec for each review\n",
    "\n",
    "sentence_vectors = [] # the average word2vec for each review/sentence is stored in sentence_vect\n",
    "for sentence in list_of_sentence:  # for each review/sentence\n",
    "    sent_vect = np.zeros(50)   # as words vectors are of zero length\n",
    "    count_words = 0  # no of words with a valid vector in the sentence/review\n",
    "    for word in sentence:  # for each word in a review/sentence\n",
    "        try:\n",
    "            vector = w2v_model.wv[word]\n",
    "            sent_vect += vector\n",
    "            count_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vect = sent_vect / count_words\n",
    "    sentence_vectors.append(sent_vect)\n",
    "print(len(sentence_vectors))\n",
    "print(len(sentence_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf word2vec\n",
    "tfidf_feature = tf_idf_vect.get_feature_names()  # tfidf words/col-names\n",
    "#final-tf_idf is the sparse matrix with row = sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sentence_vectors = []# the tfidf-w2v for eah sentence/review is stored in the variable tfidf_sent_vectors\n",
    "row = 0\n",
    "for sentence in list_of_sentence:  # for each review/sentence\n",
    "    sent_vect = np.zeros(50)    # as words vectors are of zero length\n",
    "    weight_sum = 0   # no of words with a valid vector in the sentence/review\n",
    "    for word in sentence:  # for each word in a review/sentence\n",
    "        try:\n",
    "            vector = w2v_model.wv[word]\n",
    "            # obtaun the tfidf of a word in a sentence/word\n",
    "            tf_idf = final_tf_idf[row, tfidf_feature.index(word)]\n",
    "            sent_vect += (vector * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "        except: \n",
    "            pass\n",
    "    sent_vect = sent_vect / weight_sum\n",
    "    tfidf_sentence_vectors.append(sent_vect)\n",
    "    row +=1\n",
    "print(len(tfidf_sentence_vectors))\n",
    "print(len(tfidf_sentence_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
